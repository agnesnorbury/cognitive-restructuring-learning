---
title: "modComp study: model-based analysis of learning task data, by learning task condition"
output:
  html_document:
    html-math-method:
      method: mathjax
  #  pdf_document:
  # extra_dependencies: ["bbm"]
  # fig_caption: yes
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo=FALSE, error=TRUE, warning=FALSE, message=FALSE, fig.align='center')

# load packages
packages <- c("rstan", "dplyr", "tidyr", "bayesplot", "loo", "Rlab", "boot", "hBayesDM",
              "tidybayes", "forcats", "unikn","multicon", "ggpmisc", "patchwork", "devtools",
              "lme4", "lmerTest", "rstatix", "boot", "reshape2", "ggExtra", "ggcorrplot")
if (length(setdiff(packages, rownames(installed.packages()))) > 0 ) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only=TRUE)

# load rainCloudPlot src code
source_url("https://raw.githubusercontent.com/RainCloudPlots/RainCloudPlots/master/tutorial_R/R_rainclouds.R")

# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# what task version to filter for
task_ver <- "causal-attr-learn"
```

```{r setup_rstan}
rstan_options(auto_write = TRUE)   # write the models so we don't have to recompile each time
nCores <- parallel::detectCores()    # get number of cores available for parallelisation
```

```{r setup_colour_scales}
# lets set some custom colour scales for our plots using unikn
# seecol(pal_unikn_pair)
palette2 <- usecol(pal_unikn_pair) 
# colours by intervention group
colours3 <- c("1" = palette2[2],      # restructuring + learning training
              "2" = palette2[3],      # control + learning training
              "3" = palette2[10]      # restructuring + learning control
              )
# colours by model parameter type
colours2 <- c("group mean" = palette2[14],
              "intervention effect" = palette2[2],  # cognitive restructuring vs control intervention
              "learning effect" = palette2[9]       # learning task vs control learning task
              )
```

```{r load_data}
# load long format data
data_long <- read.csv(file=paste0("./", task_ver, "-learning-task-data-anon.csv")) %>%
  dplyr::select(-X) %>%
  arrange(uid, trialNo) %>%
  filter(timedout==FALSE)

## get number of time points etc
nPpts <- length(unique(data_long$uid))

# get lists of participant  IDs by condition for use with other data
control_subs <- data_long %>%
  filter(interventionCondition=="control") %>%
  dplyr::select(uid, ID)
controls <- as.list(unique(control_subs$uid))
control_IDs <- as.list(unique(control_subs$ID))

learn_control_subs <- data_long %>%
  filter(learningCondition=="control") %>%
  dplyr::select(uid, ID)
learn_controls <- as.list(unique(learn_control_subs$uid))
learn_control_IDs <- as.list(unique(learn_control_subs$ID))

learn_causal_subs <- data_long %>%
  filter(learningCondition=="causal") %>%
  dplyr::select(uid, ID)
learn_causals <- as.list(unique(learn_causal_subs$uid))
learn_causal_IDs <- as.list(unique(learn_causal_subs$ID))

# get ordered list of intervention conditions
int_conds <- data_long %>%
  arrange(ID) %>%
  group_by(ID) %>%
  dplyr::select(ID, interventionCondition) %>%
  distinct() %>%
  mutate(condition01 = ifelse(interventionCondition=="psychoed", 1, 0))

learn_conds <- data_long %>%
  arrange(ID) %>%
  group_by(ID) %>%
  dplyr::select(ID, learningCondition) %>%
  distinct() %>%
  mutate(condition01 = ifelse(learningCondition=="causal", 1, 0))

# get lists of participants by study arm
s1 <- data_long %>%
  filter(interventionCondition=="psychoed" & learningCondition=="causal" ) %>%
  dplyr::select(ID)
subs_1 <- as.list(unique(s1$ID))

s2 <- data_long %>%
  filter(interventionCondition=="control" & learningCondition=="causal") %>%
  dplyr::select(ID)
subs_2 <- as.list(unique(s2$ID))

s3 <- data_long %>%
  filter(interventionCondition=="psychoed" & learningCondition=="control") %>%
  dplyr::select(ID)
subs_3 <- as.list(unique(s3$ID))
```

Quick plot of the data we'll be modelling:

```{r tot_choice}
p <- data_long %>%
  dplyr::select(uid, valence, internalGlobalChosen, 
         internalSpecificChosen, externalGlobalChosen, externalSpecificChosen, learningCondition) %>%
  group_by(uid, valence) %>%
  mutate(trialNoByVal = 1:n()) %>%
  melt(id.vars=c("uid", "valence", "trialNoByVal", "learningCondition")) %>%
  ggplot(aes(x=trialNoByVal, y=value, group=variable, colour=variable, fill=variable)) +
  stat_summary(fun=mean, geom="line") +
  stat_summary(fun.data=mean_se, geom="ribbon", alpha=.2, 
                   colour = NA) +
  ylab("proportionate choice of each attribution type") +
  xlab("trial") + facet_grid(cols=vars(valence), rows=vars(learningCondition)) +
  theme_minimal() + theme(legend.position="top") + ylim(0,1) +
  geom_hline(yintercept = c(0.5), linetype = "dotted") +
  geom_vline(xintercept = c(10,20), linetype = "dashed") +
  scale_fill_brewer(palette="Set2") + scale_colour_brewer(palette="Set2")
p
```
And some linear-model analysis of choice accuracy behaviour

```{r behav_lm}
# learning training task only 
data_long_all2 <- data_long %>%
  filter(learningCondition=="causal") %>%
  group_by(uid, blockNo) %>%
  mutate(trialWithinBlock = 1:n())

lm <- lmer(correct ~ trialWithinBlock * valence * blockNo + (1 | uid),
           data = data_long_all2)
summary(lm)
anova(lm)

# control learning task only
data_long_all2 <- data_long %>%
  filter(learningCondition=="control") %>%
  group_by(uid, blockNo) %>%
  mutate(trialWithinBlock = 1:n())

lm <- lmer(correct ~ trialWithinBlock * valence * blockNo + (1 | uid),
           data = data_long_all2)
summary(lm)
anova(lm)

# both tasks together
data_long_all3 <- data_long %>%
  group_by(uid, blockNo) %>%
  mutate(trialWithinBlock = 1:n())

lm <- lmer(correct ~ trialWithinBlock * valence * blockNo * learningCondition + (1 | uid),
           data = data_long_all3)
summary(lm)
anova(lm)
```

And a look at choice reaction times (RTs), by learning task condition

```{r tot_rt}
p <- data_long %>%
  filter(timedout==FALSE) %>%
  dplyr::select(uid, valence, trialNo, rt, learningCondition) %>%
  group_by(uid, valence) %>%
  mutate(trialNoByVal = 1:n()) %>%
  ggplot(aes(x=trialNoByVal, y=rt, group=valence, colour=valence, fill=valence)) +
  stat_summary(fun=mean, geom="line") +
  stat_summary(fun.data=mean_se, geom="ribbon", alpha=.2, 
                   colour = NA) +
  ylab("mean RT") +
  xlab("trial") + 
  theme_minimal() + theme(legend.position="top") +
  geom_vline(xintercept = c(10,20), linetype = "dashed") +
  scale_fill_brewer(palette="Set2") + scale_colour_brewer(palette="Set2") + facet_wrap(~learningCondition)
p
```

```{r rt_lm}
# causal (learning training) task only 
data_long_all2 <- data_long %>%
  filter(learningCondition=="causal") %>%
  group_by(uid, blockNo) %>%
  mutate(trialWithinBlock = 1:n())

lm <- lmer(rt ~ trialNo * valence * blockNo + (1 | uid),
           data = data_long_all2)
summary(lm)
anova(lm)

lm <- lmer(rt ~ trialWithinBlock * valence * blockNo + (1 | uid),
           data = data_long_all2)
summary(lm)
anova(lm)

# both tasks
data_long_all3 <- data_long %>%
  group_by(uid, blockNo) %>%
  mutate(trialWithinBlock = 1:n())

lm <- lmer(rt ~ trialWithinBlock * valence * blockNo * learningCondition + (1 | uid),
           data = data_long_all3)
summary(lm)
anova(lm)
```


### Q-learning models

Here, we will base our analysis on the best model from study 1 model comparison and SBC analysis.

Since there are some differences in behaviour between the task types, we will see if a version that allows different group-level means for task parameters between learning task conditions does better for this case.

```{r rstan_fit_2opt}
# 1. best model from study 1 analysis:
# model <- "m_qlearning_negpos_2alpha_2q0i1_2q0i23"    
# 2. same model, with separate group parameter means for learning training and control learning tasks
model <- "m_qlearning_negpos_2alpha_2q0i1_2q0i23_bylearn" 

## define data to fit
data_to_fit <- "b123"
nTrials_max <- 60

## select data to model
data_long_b123 <- data_long %>%
  dplyr::select(uid, ID, trialNo, itemNo, blockNo, valence, chosen_attr_type, correct, rt) %>%
  mutate(valence01 = ifelse(valence == "negative", 0, 
                        ifelse(valence == "positive", 1, NA)),
         choiceIG = ifelse(chosen_attr_type == "internal_global", 2, 1),
         choiceIG01 = ifelse(chosen_attr_type == "internal_global", 1, 0)) %>%
  group_by(uid) %>%
  mutate(newTrialNo = 1:n()) %>%
  ungroup()
# for block1, int_spec = 1 int_glob = 2
# for block2, ext_glob = 1 int_glob = 2 
# for block3, ext_spec = 1 int_glob = 2 

# create arrays of choice options and responses for each participant [and time point]
blockNo <- valence <- choiceIG <- choiceIG01 <- outcome <- array(0, dim = c(nPpts, nTrials_max))
nT_ppts <- array(nTrials_max, dim = c(nPpts))
for (p in 1:nPpts) {
  blockNo[p,]   <- with(eval(as.symbol(paste0("data_long_", data_to_fit))), blockNo[ID==p])
  valence[p,]   <- with(eval(as.symbol(paste0("data_long_", data_to_fit))), valence01[ID==p])
  choiceIG[p,]  <- with(eval(as.symbol(paste0("data_long_", data_to_fit))), choiceIG[ID==p])
  choiceIG01[p,]<- with(eval(as.symbol(paste0("data_long_", data_to_fit))), choiceIG01[ID==p])
  outcome[p,]   <- with(eval(as.symbol(paste0("data_long_", data_to_fit))), correct[ID==p])
}

# create data list to pass to stan
data_list = list(
  nPpts = nPpts,
  nTrials_max = nTrials_max,     # max number of trials [per session] per participant
  nT_ppts = nT_ppts,             # actual number of trials [per session] per participant
  learn_cond = learn_conds$condition01,
  blockLength = nTrials_max/max(blockNo),
  blockNo = blockNo,
  valence = valence,
  nChoices = 2,
  choice = choiceIG,
  outcome = outcome
)

# fit model using rstan
fit <- stan(
  file = paste0("./stan-models/", model, ".stan"),
  data = data_list,
  chains = 4,               # run 4 separate chains to assess convergence
  warmup = 1000,            # these are used to tune the sampler and ’burn in’
  iter = 2000,              # number of iterations (#kept = chains*(iter - warmup))
  cores = nCores            # chains to be run in parallel on separate cores (if possible)
)

## save
saveRDS(fit, file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))
## OR load saved model fit
# fit <- readRDS(file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))

# check overall dx
check_hmc_diagnostics(fit)

# plot pairs of pars to examine non-identifiability:
pairs(fit, pars=c("alpha_neg[1]", "alpha_pos[1]", "beta[1]"))

pairs(fit, pars=c("alpha_neg[1]", "alpha_pos[1]", "beta[1]",
                  "q0_1_IG_neg[1]", "q0_1_IG_pos[1]", "q0_23_IG_neg[1]", "q0_23_IG_pos[1]"))


```

```{r mc}
## comparison between models
models_to_compare <- c("m_qlearning_negpos_2alpha_2q0i1_2q0i23",
                       "m_qlearning_negpos_2alpha_2q0i1_2q0i23_bylearn"
                       )
# we can use the loo_compare function to compare our two models on expected log predictive density (ELPD) for new data
data_to_fit <- "b123" 
for (i in 1:length(models_to_compare)) {
  fit <- readRDS(paste0("./stan-fits/",models_to_compare[i],"-",data_to_fit, "-",task_ver,"-fit.rds"))
  assign(paste0("loo",as.character(i)), loo(fit, mc.cores = nCores)) 
}
loo_compare(loo1, loo2) 
# in this output, the best model is defined as having zero difference to itself
# see e.g., https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html
```


```{r plot_posteriors}
## define model and data to fit
model <- "m_qlearning_negpos_2alpha_2q0i1_2q0i23_bylearn"
data_to_fit <- "b123"
fit <- readRDS(file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))

# plot posterior densities for key parameters
a <- plot(fit, pars = "alpha_neg", ci_level = 0.5, fill_color = "sky blue") +
  ggtitle("posterior alpha_neg") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)
  
b <- plot(fit, pars = "alpha_pos", ci_level = 0.5, fill_color = "dark blue") +
  ggtitle("posterior alpha_pos") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)

c <- plot(fit, pars = "beta", ci_level = 0.5, fill_color = "sky blue") +
  ggtitle("posterior beta") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5))
(a + b + c)

a <- plot(fit, pars = "q0_1_IG_neg", ci_level = 0.5, fill_color = "sky blue") +
  ggtitle("posterior q0_1_IG_neg") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)

b <- plot(fit, pars = "q0_1_IG_pos", ci_level = 0.5, fill_color = "dark blue") +
  ggtitle("posterior q0_1_IG_pos") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)

c <- plot(fit, pars = "q0_23_IG_neg", ci_level = 0.5, fill_color = "sky blue") +
  ggtitle("posterior q0_23_IG_neg") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)

d <- plot(fit, pars = "q0_23_IG_pos", ci_level = 0.5, fill_color = "dark blue") +
  ggtitle("posterior q0_23_IG_pos") +
  theme(axis.text.y=element_blank(), plot.title=element_text(size = 12, face = "bold", hjust = 0.5)) +
  xlim(0,1)

(a + b)
(c + d)
```

```{r pred_acc}
# original data
y <- data_long_b123$choiceIG

# extract replicate data generated using posterior parameter estimates
y_rep <- as.data.frame(summary(fit, pars = c("y_rep"))$summary) %>%
  filter(mean>=0) %>%   #remove padded values for trials ppts didn't actually complete
  dplyr::select(`50%`) %>%
  add_rownames(var = "var") %>%
  separate(var, sep="\\[", into=c("variable", "tmp"), remove=TRUE, extra="drop") %>%
  separate(tmp, sep=",", into=c("ID", "trial"), remove=TRUE, extra="drop") %>%
  separate(trial, sep=-1, into="trial", extra="drop") %>%
  mutate(ID=as.numeric(ID)) %>%
  arrange(ID) %>%
  rename(mean_predicted = `50%`)

# predictive accuracy (per sub)
y_rep <- cbind(y_rep, observed=y)
accs <- y_rep %>%
  group_by(ID) %>%
  mutate(acc = (mean_predicted==observed)) %>%
  summarise(mean_acc = mean(acc))

# predictive accuracy (overall)
summ <- accs %>%
  summarise(mean_pred_acc=round(mean(mean_acc),2), sd_pred_acc=round(sd(mean_acc),2))
print(summ)

# sum log lik for each participant
sum_log_liks <- rstan::extract(fit, pars = "log_lik")[[1]]
mean_sum_log_liks <- colMeans(sum_log_liks) # average over samples

# pseudo r2
# pseudo-r2 can be calculated as 1 - L/C where L = sumloglik over participants and C = likelihood of observing by chance (nTrials*log(0.5))
pseudo_r2 = 1 - ( mean_sum_log_liks / (nTrials_max*log(0.5)) )
print(mean(pseudo_r2))

# for learning training ppts only:
pred_acc_mean_control <- mean(accs$mean_acc[unlist(learn_causal_IDs)]); print(pred_acc_mean_control)
pred_acc_sd_control <- sd(accs$mean_acc[unlist(learn_causal_IDs)]); print(pred_acc_sd_control)
msll_control <- mean_sum_log_liks[unlist(learn_causal_IDs)]
pseudo_r2 = 1 - ( msll_control / (nTrials_max*log(0.5)) )
print(mean(pseudo_r2))

# for control learning ppts only:
pred_acc_mean_control <- mean(accs$mean_acc[unlist(learn_control_IDs)]); print(pred_acc_mean_control)
pred_acc_sd_control <- sd(accs$mean_acc[unlist(learn_control_IDs)]); print(pred_acc_sd_control)
msll_control <- mean_sum_log_liks[unlist(learn_control_IDs)]
pseudo_r2 = 1 - ( msll_control / (nTrials_max*log(0.5)) )
print(mean(pseudo_r2))
```

Are learning rates associated with (separately-modelled) changes in attribution tendencies on the causal attribution task?

```{r predict_thetas}
model <- "m_qlearning_negpos_2alpha_2q0i1_2q0i23_bylearn"
data_to_fit <- "b123"
fit <- readRDS(file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))

params = c("alpha_pos", "alpha_neg", "beta")
learning_posts <- as.data.frame(summary(fit, pars=params)$summary) %>%
  dplyr::select(mean,sd) %>%
  add_rownames(var = "var") %>%
  separate(var, sep="\\[", into=c("parameter","uid"), remove=TRUE, extra="drop") %>%
  separate(uid, sep=-1, into="uid", extra="drop")

# specify choice data model
# ! here, we will use the causal attribution task model that *doesn't* know about learning task condition, in order to avoid biasing results
model_choice <- "m_bernoulli_negpos_IGcorr2_multisess_intervention_additive" 
fit_choice <- readRDS(file = paste0("./stan-fits/", model_choice ,"-", task_ver, "-fit.rds"))

# extract posteriors
choice_posts <- as.data.frame(summary(fit_choice, pars = c("p_internal_pos", "p_internal_neg",
                                               "p_global_pos", "p_global_neg"))$summary) %>%
    dplyr::select(mean, sd) %>%
    add_rownames(var = "var") %>%
    separate(var, sep="\\[|\\,|\\]", into=c("parameter", "uid", "session"), 
             remove=TRUE, extra="drop") 

choice_deltas <- choice_posts %>%
  pivot_wider(id_cols=c("uid", "session"), 
              names_from = "parameter", values_from = c("mean", "sd")) %>%
  arrange(uid, session) %>%
  group_by(uid) %>%
  mutate(delta_int_p_neg  = mean_p_internal_neg[session==2] - mean_p_internal_neg[session==1],
         delta_int_p_pos  = mean_p_internal_pos[session==2] - mean_p_internal_pos[session==1],
         delta_glob_p_neg = mean_p_global_neg[session==2] - mean_p_global_neg[session==1],
         delta_glob_p_pos = mean_p_global_pos[session==2] - mean_p_global_pos[session==1]
         ) %>%
  #dplyr::select(uid, contains("delta_")) %>%
  distinct()

# merge together choice and learning model params (separate fits)
learning_params <- learning_posts %>%
  pivot_wider(values_from = c("mean", "sd"), names_from = "parameter", id_cols = "uid")

choice_learning <- merge(choice_deltas, learning_params, by="uid")

# plot by full group
p3 <- choice_learning %>%
  mutate(precision = 1/sd_alpha_pos,
         int_condition = ifelse(uid %in% control_IDs, "control", "psychoeducation"),
         learn_condition = ifelse(uid %in% learn_control_IDs, "control", "causal"),
         group = ifelse(uid %in% subs_1, 1,
                        ifelse(uid %in% subs_2, 2,
                               ifelse(uid %in% subs_3, 3, NA))),
         group = as.factor(group)) %>%
  ggplot(aes(x=mean_alpha_pos, y=delta_int_p_pos, size = precision, weight=precision, 
             group=group, colour=group, fill=group)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  scale_colour_manual(values=colours3) + 
  scale_fill_manual(values=colours3) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE) +
  theme_minimal() + scale_size(guide = 'none') + theme(legend.position="right") +
  facet_wrap(~learn_condition, scales = "free")
p3
# ggsave(filename = paste0("./figures/", task_ver, "-", model, "-int-params-by-time-nosf.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

p4 <- choice_learning %>%
  mutate(precision = 1/sd_alpha_pos,
         int_condition = ifelse(uid %in% control_IDs, "control", "psychoeducation"),
         learn_condition = ifelse(uid %in% learn_control_IDs, "control", "causal"),
         group = ifelse(uid %in% subs_1, 1,
                        ifelse(uid %in% subs_2, 2,
                               ifelse(uid %in% subs_3, 3, NA))),
         group = as.factor(group)) %>%
  ggplot(aes(x=mean_alpha_pos, y=delta_glob_p_pos, size = precision, weight=precision, 
             group=group, colour=group, fill=group)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  scale_colour_manual(values=colours3) + 
  scale_fill_manual(values=colours3) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE) +
  theme_minimal() + scale_size(guide = 'none') + theme(legend.position="right") +
  facet_wrap(~learn_condition, scales = "free")
p4
# ggsave(filename = paste0("./figures/", task_ver, "-", model, "-glob-params-by-time-nosf.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)
```

We can test if these simple correlations are different using Fisher's R-to-Z tests

```{r lme}
# quickly rearrange data
choice_learning2 <- choice_learning %>%
  mutate(uid = as.numeric(uid),
         int_condition = ifelse(uid %in% control_IDs, "control", "psychoeducation"),
         learn_condition = ifelse(uid %in% learn_control_IDs, "control", "causal")) %>%
  arrange(uid) %>%
  filter(learn_condition=="causal")

# get weighted correlation estimates:
wts <- choice_learning3 %>%
  filter(learn_condition=="causal") 
tmp <- choice_learning3 %>%
  filter(learn_condition=="causal")  %>%
  dplyr::select(mean_alpha_pos, delta_int_p_pos, delta_glob_p_pos)
weighted_corr <- cov.wt(tmp, wt = 1/wts$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor

wts <- choice_learning3 %>%
  filter(learn_condition=="control") 
tmp <- choice_learning3 %>%
  filter(learn_condition=="control")  %>%
  dplyr::select(mean_alpha_pos, delta_int_p_pos, delta_glob_p_pos)
weighted_corr <- cov.wt(tmp, wt = 1/wts$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor

paired.r(0.20, 0.10, n=164)

paired.r(0.22, 0.11, n=164)

wts <- choice_learning3 %>%
  filter(learn_condition=="causal" & int_condition =="psychoeducation") 
tmp <- choice_learning3 %>%
  filter(learn_condition=="causal" & int_condition =="psychoeducation")  %>%
  dplyr::select(mean_alpha_pos, delta_int_p_pos, delta_glob_p_pos)
weighted_corr <- cov.wt(tmp, wt = 1/wts$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor

wts <- choice_learning3 %>%
  filter(learn_condition=="causal" & int_condition =="control") 
tmp <- choice_learning3 %>%
  filter(learn_condition=="causal" & int_condition =="control")  %>%
  dplyr::select(mean_alpha_pos, delta_int_p_pos, delta_glob_p_pos)
weighted_corr <- cov.wt(tmp, wt = 1/wts$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor

paired.r(0.12, 0.22, n=164)

paired.r(0.25, 0.18, n=164)
```

So ot again looks like there might be some association - what if we fit the data jointly?

```{r arrange_data_joint}
# rejig the data so we have everything we need
data_long_all_learning <- data_long %>%
  dplyr::select(uid, ID, trialNo, itemNo, blockNo, valence, chosen_attr_type, correct, rt) %>%
  mutate(valence01 = ifelse(valence == "negative", 0, 
                        ifelse(valence == "positive", 1, NA)),
         choiceIG = ifelse(chosen_attr_type == "internal_global", 2, 1),
         choiceIG01 = ifelse(chosen_attr_type == "internal_global", 1, 0)) %>%
  group_by(uid) %>%
  mutate(newTrialNo = 1:n()) %>%
  ungroup() %>%
  arrange(uid)

data_long_all_choice <- read.csv(file=paste0(task_ver, "-causal-attribution-task-data-anon.csv")) %>%
  arrange(uid) %>%
  mutate(sess = taskNo +1,
         neg_pos = ifelse(valence == "negative", 0, 1))

# check everything is aligned properly
nPpts_c <- length(unique(data_long_all_learning$uid))
nPpts_l <- length(unique(data_long_all_choice$uid))
nPpts_c==nPpts_l
unique(data_long_all_learning$uid)==unique(data_long_all_choice$uid)
unique(data_long_all_learning$ID)==unique(data_long_all_choice$ID)

# organise the data for rstan
nPpts <- nPpts_c
nTrials_max_l <- max(data_long_all_learning$newTrialNo)
nTrials_max_c <- max(data_long_all_choice$nTrials)
nTimes <- max(data_long_all_choice$sess)
  
blockNo <- valence <- choiceIG <- choiceIG01 <- outcome <- array(0, dim = c(nPpts, nTrials_max_l))
nT_ppts_l <- array(nTrials_max_l, dim = c(nPpts))

internalChosen_neg <- internalChosen_pos <- globalChosen_neg <- globalChosen_pos <- array(0, dim = c(nPpts, nTimes, nTrials_max_c/2))
nT_ppts_c <- array(nTrials_max_c, dim = c(nPpts, nTimes))

for (p in 1:nPpts) {
  blockNo[p,]   <- with(data_long_all_learning, blockNo[ID==p])
  valence[p,]   <- with(data_long_all_learning, valence01[ID==p])
  choiceIG[p,]  <- with(data_long_all_learning, choiceIG[ID==p])
  choiceIG01[p,]<- with(data_long_all_learning, choiceIG01[ID==p])
  outcome[p,]   <- with(data_long_all_learning, correct[ID==p])
  
  for (t in 1:nTimes) {
    internalChosen_neg[p,t,] <- with(data_long_all_choice, internalChosen[ID==p & sess==t & neg_pos==0])
    internalChosen_pos[p,t,] <- with(data_long_all_choice, internalChosen[ID==p & sess==t & neg_pos==1])
    globalChosen_neg[p,t,] <- with(data_long_all_choice, globalChosen[ID==p & sess==t & neg_pos==0])
    globalChosen_pos[p,t,] <- with(data_long_all_choice, globalChosen[ID==p & sess==t & neg_pos==1])
  }
}

# create data list to pass to stan
data_list = list(
  # overall data
  nPpts = nPpts,
  nTimes = nTimes,
  # learning task data
  nTrials_max_l = nTrials_max_l,     # max number of trials [per session] per participant
  nT_ppts_l = nT_ppts_l,             # actual number of trials [per session] per participant
  blockNo = blockNo,
  valence = valence,
  nChoices_l = 2,
  choice = choiceIG,
  outcome = outcome,
  # causal attribution task data
  nTrials_max_c = nTrials_max_c/2,         # max number of trials per  session per participant
  nT_ppts_c = nT_ppts_c/2,                 # actual number of trials per session per participant
  int_condition = int_conds$condition01,       # 0 = control, 1 = psychoed
  learn_condition = learn_conds$condition01,       # 0 = control, 1 = psychoed
  internal_neg = internalChosen_neg,
  internal_pos = internalChosen_pos,
  global_neg = globalChosen_neg,
  global_pos = globalChosen_pos
)
```

Again, we can fit 2 joint models: one with overall weights for (positive) learning rates from the learning tasks (separately for each learning task condition), and one testing for an additional effect of learning training in participants who received cognitive restructuring

```{r fit_joint_1}
# joint model 1:
model <- "m_bernoulli_negpos_IGcorr2_multisess_intervention_additive_joint_Qlearning4_bothg_IG_seplearning"

## fit model using rstan
fit <- stan(
  file = paste0("./stan-models/", model, ".stan"),
  data = data_list,
  chains = 4,               # run 4 separate chains to assess convergence
  warmup = 1000,            # these are used to tune the sampler and ’burn in’
  iter = 2000,              # number of iterations (#kept = chains*(iter - warmup))
  cores = nCores            # chains to be run in parallel on separate cores (if possible)
)

# save
saveRDS(fit, file = paste0("./stan-fits/", model ,"-", task_ver, "-fit.rds"))
# ## OR load saved model fit
# fit <- readRDS(file = paste0("./stan-fits/", model ,"-", task_ver, "-fit.rds"))

# summary of sampling diagnostics
check_hmc_diagnostics(fit)

# plot group-level effects of interest using tidybayes
fit_tidy <- fit %>%
  gather_draws(beta_causal_internal, beta_causal_global, 
               beta_control_internal, beta_control_global) %>%
  mutate(
         var_type = ifelse(grepl("_int_", .variable), "intervention effect",
                           ifelse(grepl("beta_", .variable), "learning", "group mean")),
         var_type = factor(var_type, levels = c("group mean", "intervention effect", "learning")),
         .variable = factor(.variable, levels = c(
           "beta_causal_internal", "beta_causal_global",
           "beta_control_internal", "beta_control_global"
           )))

p <- fit_tidy %>%
  ggplot(aes(y = fct_rev(.variable), x = .value, fill = var_type)) +
  stat_gradientinterval(.width = c(.9, .5),  slab_size = 1) +
  scale_fill_manual(values = colours2) +
  geom_vline(xintercept = 0, colour = "grey") + theme_minimal() +
  theme(legend.position = "none") + theme(aspect.ratio=4/3) + labs(x="", y="")
p
# ggsave(filename = paste0("./figures/", task_ver, "-", model, "-post-CIsy.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

# extract numerical values of group-level parameters (raw / unstandardized)
params90cis <- summary(fit, pars = c("theta_int_internal_neg", "theta_int_internal_pos",
                                     "theta_int_global_neg", "theta_int_global_pos",
                                     "theta_learn_internal_neg", "theta_learn_internal_pos",
                                     "theta_learn_global_neg", "theta_learn_global_pos",
                                     "beta_causal_internal", "beta_causal_global",
                                     "beta_control_internal", "beta_control_global"),
                       probs = c(0.05, 0.95))$summary
print(params90cis)

## new plotting (standardized)
## convert to standardized, e.g.:
# beta_int1_std = sqrt(sigma_thetas[1]) / sqrt(pars_sigma_pos[3]) * beta_int1
# first, get posterior (pooled) variance estimates for theta_internal_pos at each time point
params90cis <- summary(fit, pars = c("pars_sigma_neg[3]",
                                     "pars_sigma_neg[4]", 
                                     "pars_sigma_pos[3]",
                                     "pars_sigma_pos[4]"), probs = c(0.05, 0.95))$summary
sigma_theta_int_pos_t2 <- params90cis[3,1]
sigma_theta_glob_pos_t2 <- params90cis[4,1]
# for alphas, which are non hierarchically estimates, we will have to take the SD across posterior mean estimates
tmp  <- summary(fit, pars = "alpha_pos", probs = c(0.05, 0.95))$summary
sd_alpha_pos <- sd(tmp[,1])

fit_tidy2 <- fit_tidy %>%
  mutate(.value2 = case_when(.variable =="beta_causal_internal" ~ sd_alpha_pos/sqrt(sigma_theta_int_pos_t2) *.value,
                              .variable =="beta_causal_global" ~ sd_alpha_pos/sqrt(sigma_theta_glob_pos_t2) *.value,
                              .variable =="beta_control_internal" ~ sd_alpha_pos/sqrt(sigma_theta_int_pos_t2) *.value,
                              .variable =="beta_control_global" ~ sd_alpha_pos/sqrt(sigma_theta_glob_pos_t2) *.value,
                             TRUE ~ .value))
p2 <- fit_tidy2 %>%
  ggplot(aes(y = fct_rev(.variable), x = .value2, fill = var_type)) +
  stat_gradientinterval(.width = c(.9, .5),  slab_size = 1) +
  scale_fill_manual(values = colours2) +
  geom_vline(xintercept = 0, colour = "grey") + theme_minimal() + #xlim(-1,7) +
  theme(legend.position = "none") + theme(aspect.ratio=2/6) + labs(x="", y="") +
  theme(axis.text.x = element_text(size = 10))
p2
# ggsave(filename = paste0("./figures/", task_ver, "-", model,
#                          "-joint-model-1-smd.svg"),
#       plot = last_plot(), device = "svg", dpi = 300)
```

```{r fit_joint_2}
# joint model 2:
model <- "m_bernoulli_negpos_IGcorr2_multisess_intervention_additive_joint_Qlearning4_bothg_IG_seplearning_activ2"

## fit model using rstan
fit <- stan(
  file = paste0("./stan-models/", model, ".stan"),
  data = data_list,
  chains = 4,               # run 4 separate chains to assess convergence
  warmup = 1000,            # these are used to tune the sampler and ’burn in’
  iter = 2000,              # number of iterations (#kept = chains*(iter - warmup))
  cores = nCores            # chains to be run in parallel on separate cores (if possible)
)

# save
saveRDS(fit, file = paste0("./stan-fits/", model ,"-", task_ver, "-fit.rds"))
# ## OR load saved model fit
# fit <- readRDS(file = paste0("./stan-fits/", model ,"-", task_ver, "-fit.rds"))

# summary of sampling diagnostics
check_hmc_diagnostics(fit)

# plot group-level posteriors using tidybayes
fit_tidy <- fit %>%
  gather_draws(beta_causal_internal, beta_causal_global, 
               beta_causal_activ_internal, beta_causal_activ_global, 
               beta_control_internal, beta_control_global
               ) %>%
  mutate(
         var_type = ifelse(grepl("_int_", .variable), "intervention effect",
                           ifelse(grepl("beta_", .variable), "learning", "group mean")),
         var_type = factor(var_type, levels = c("group mean", "intervention effect", "learning")),
         .variable = factor(.variable, levels = c(
           "beta_causal_internal", "beta_causal_global",
           "beta_causal_activ_internal", "beta_causal_activ_global",
           "beta_control_internal", "beta_control_global"
           )))

p <- fit_tidy %>%
  ggplot(aes(y = fct_rev(.variable), x = .value, fill = var_type)) +
  stat_gradientinterval(.width = c(.9, .5),  slab_size = 1) +
  scale_fill_manual(values = colours2) +
  geom_vline(xintercept = 0, colour = "grey") + theme_minimal() +
  theme(legend.position = "none") + theme(aspect.ratio=4/3) + labs(x="", y="") #+ xlim(-1,4)
p
# ggsave(filename = paste0("./figures/", task_ver, "-", model, "-post-CIs-bothg-activ.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

# extract numerical values (raw / unstandardized)
params90cis <- summary(fit, pars = c("theta_int_internal_neg", "theta_int_internal_pos",
                                     "theta_int_global_neg", "theta_int_global_pos",
                                     "theta_learn_internal_neg", "theta_learn_internal_pos",
                                     "theta_learn_global_neg", "theta_learn_global_pos",
                                     "beta_causal_internal", "beta_causal_global",
                                     "beta_causal_activ_internal", "beta_causal_activ_global",
                                     "beta_control_internal", "beta_control_global"),
                       probs = c(0.05, 0.95))$summary
print(params90cis)

## new plotting (standardized)
## convert to standardized, e.g.:
# beta_int1_std = sqrt(sigma_thetas[1]) / sqrt(pars_sigma_pos[3]) * beta_int1
# first, get posterior (pooled) variance estimates for theta_internal_pos at each time point
params90cis <- summary(fit, pars = c("pars_sigma_neg[3]",
                                     "pars_sigma_neg[4]", 
                                     "pars_sigma_pos[3]",
                                     "pars_sigma_pos[4]"), probs = c(0.05, 0.95))$summary
sigma_theta_int_pos_t2 <- params90cis[3,1]
sigma_theta_glob_pos_t2 <- params90cis[4,1]
# for betas, which are non hierarchically estimates, we will have to take the SD across posterior mean estimates
tmp  <- summary(fit, pars = "alpha_pos", probs = c(0.05, 0.95))$summary
sd_alpha_pos <- sd(tmp[,1])

fit_tidy2 <- fit_tidy %>%
  mutate(.value2 = case_when(.variable =="beta_causal_internal" ~ sd_alpha_pos/sqrt(sigma_theta_int_pos_t2) *.value,
                              .variable =="beta_causal_global" ~ sd_alpha_pos/sqrt(sigma_theta_glob_pos_t2) *.value,
                              .variable =="beta_causal_activ_internal" ~ sd_alpha_pos/sqrt(sigma_theta_int_pos_t2) *.value,
                              .variable =="beta_causal_activ_global" ~ sd_alpha_pos/sqrt(sigma_theta_glob_pos_t2) *.value,
                              .variable =="beta_control_internal" ~ sd_alpha_pos/sqrt(sigma_theta_int_pos_t2) *.value,
                              .variable =="beta_control_global" ~ sd_alpha_pos/sqrt(sigma_theta_glob_pos_t2) *.value,
                             TRUE ~ .value))
p2 <- fit_tidy2 %>%
  ggplot(aes(y = fct_rev(.variable), x = .value2, fill = var_type)) +
  stat_gradientinterval(.width = c(.9, .5),  slab_size = 1) +
  scale_fill_manual(values = colours2) +
  geom_vline(xintercept = 0, colour = "grey") + theme_minimal() + #xlim(-1,7) +
  theme(legend.position = "none") + theme(aspect.ratio=2/6) + labs(x="", y="") +
  theme(axis.text.x = element_text(size = 10))
p2
# ggsave(filename = paste0("./figures/", task_ver, "-", model,
#                          "-joint-model-2-smd.svg"),
#       plot = last_plot(), device = "svg", dpi = 300)
```

Are (positive) learning rates from the learning task related to other learning task data (ratings, free text classification label probabilities)?

1. Correlations with ratings data

```{r ratings}
data_r_long_all <- read.csv(file = paste0(task_ver, "-learning-task-ratings-data-anon.csv")) %>%
  dplyr::select(-X)
```

First, let's look at the ratings data

```{r ratings_plot}
p1 <- data_r_long_all %>%
  melt(id.vars=c("uid", "block", "valence")) %>%
  group_by(variable, valence, block) %>%
  summarise(mean = mean(value),
            sd = sd(value),
            se = sd(value)/sqrt(nPpts)) %>%
  ggplot(aes(x=block, y=mean, group=valence, fill=valence)) +
  geom_hline(yintercept = c(50), linetype = "dotted") +
  geom_bar(stat="Identity", position=position_dodge2(0.8), width=0.5) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),
                colour="black", position = position_dodge2(width=0.4, padding=0.4), width=0.5) +
  theme_minimal() + theme(legend.position = "top") + ylim(0,100) + xlab("post-block") + 
  ylab("mean (se) rating") + facet_wrap(~variable) +
  #scale_x_discrete(labels=c("last2weeks", "int-ext", "glob-spec","glob-spec")) +
  theme(axis.text.x = element_text(angle = 45, vjust=1, hjust=1)) + 
  scale_fill_brewer(palette="Set2") + scale_colour_brewer(palette="Set2")
p1

labs <- c("external-internal", "specific-global")
names(labs) <- c("ext_int", "spec_glob")
rp1 <- data_r_long_all %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  melt(id.vars=c("uid", "block", "valence", "learnCond")) %>%
  mutate(valence=recode(valence, `neg`="negative",`pos`="positive"),
         block=recode(block,
                      `block0` = "pre-task",
                      `block1` = "scenario 1",
                      `block2` = "scenario 2",
                      `block3` = "scenario 3")) %>%
  ggplot(aes(x=block, y=value, group=interaction(block,valence), fill=valence, 
             colour=valence)) +
  geom_hline(yintercept = c(50), linetype = "dotted") +
  geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
  geom_point(position=position_jitter(width=.1), size = 2, shape = 20) +
  geom_line(aes(group=interaction(uid,block)), colour="grey", alpha = .4) +
  geom_boxplot(outlier.shape = NA, alpha = .5, width = .3, colour = "black") +
  theme_minimal() + ylim(0,100) +
  labs(x="", y="explicit cause rating") +
  theme(legend.title = element_blank()) + theme(legend.position="top") +
  facet_wrap(~blockNo, nrow=1) + facet_wrap(~variable, labeller = labeller(variable = labs)) +
  #scale_x_discrete(labels=c("last2weeks", "int-ext", "glob-spec","glob-spec", "int-ext")) +
  scale_fill_brewer(palette="Set2") + scale_colour_brewer(palette="Set2") + 
  facet_grid(cols=vars(variable), rows=vars(learnCond))
rp1
```

We can also run some linear models to check ratings are sensitive to task contingencies (ground truth)


```{r ratings_lm}
# first, just for the learning training task 
data_r_long_all2 <- data_r_long_all %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learnCond=="causal")

# for the internal-external ratings:  
lm <- lmer(ext_int ~ valence * block + (1 | uid),
           data = data_r_long_all2)
summary(lm)
anova(lm)

# for the global-specific ratings:
lm <- lmer(spec_glob ~ valence * block + (1 | uid),
           data = data_r_long_all2)
summary(lm)
anova(lm)

# for both ratings together
data_r_long_all3 <- data_r_long_all %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(!block=="block0" & learnCond=="causal") %>%
  dplyr::select(-learnCond) %>%
  melt(id_cols=c("uid", "valence", "block"))

lm <- lmer(value ~ variable * valence * block + (1 | uid),
           data = data_r_long_all3)
summary(lm)
anova(lm)

# for both tasks together
data_r_long_all4 <- data_r_long_all %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(!block=="block0") %>%
  melt(id_cols=c("uid", "valence", "block", "learnCond"))

lm <- lmer(value ~ variable * valence * block * learnCond + (1 | uid),
           data = data_r_long_all4)
summary(lm)
anova(lm)
```

Then, looking at correlations between ratings and point estimaes of learning rates.

For now, we will just look at data from participants who completed the learning training task, since control learning task ratings show little variance.

```{r ratings_corr}
data_r_wider_all <- data_r_long_all %>%
  pivot_wider(id_cols = c("uid", "block"), values_from = c("ext_int", "spec_glob"), 
              names_from = c("valence")) %>%
  arrange(uid)

# get key linking numeric sequential IDs in model outtput back to original prolific identifiers
ID_trans <- data_long %>%
  dplyr::select(uid, ID) %>%
  distinct()

# merge sx into posts data
model <- "m_qlearning_negpos_2alpha_2q0i1_2q0i23_bylearn"
data_to_fit <- "b123"
fit <- readRDS(file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))

params <- c("alpha_neg", "alpha_pos", "beta", "q0_1_IG_pos")

learning_posts <- as.data.frame(summary(fit, pars=params)$summary) %>%
  dplyr::select(mean,sd) %>%
  add_rownames(var = "var") %>%
  separate(var, sep="\\[", into=c("parameter","ID"), remove=TRUE, extra="drop") %>%
  separate(ID, sep=-1, into="ID", extra="drop") %>%
  pivot_wider(id_cols = "ID", names_from = "parameter", values_from = c("mean", "sd"))

posts_uids <- merge(learning_posts, ID_trans, by="ID")
posts_r <- merge(posts_uids, data_r_wider_all, by = "uid") %>%
  mutate(int_condition = ifelse(ID %in% control_IDs, "control", "psychoeducation"),
         learn_condition = ifelse(ID %in% learn_control_IDs, "control", "causal"))

# internal-external dimension  
p <- posts_r %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  filter(learn_condition == "causal") %>%
  ggplot(aes(x=mean_alpha_pos, y=ext_int_pos, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "bottom") +
  theme_minimal() + theme(legend.position="none") + 
  facet_wrap(~block)
p

# global-specific dimension
p <- posts_r %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  filter(learn_condition == "causal") %>%
  ggplot(aes(x=mean_alpha_pos, y=spec_glob_pos, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "bottom") +
  theme_minimal() + theme(legend.position="none") + 
  facet_wrap(~block)
p
```

We can also analyse these more formally via lmes, accounting for other parameter values

```{r ratings_lm}
# causal task only
posts_r_2 <- posts_r %>%
  filter(!block=="block0") %>%
  filter(learn_condition == "causal") %>%
  mutate(b = ifelse(block=="block1", 1,
                    ifelse(block=="block2", 2, 3)),
         ext_int_pos_z = scale(ext_int_pos),
         spec_glob_pos_z = scale(spec_glob_pos),
         mean_alpha_pos_z = scale(mean_alpha_pos),
         mean_beta_z = scale(mean_beta))

# for the internal-external dimension
lm <- lmer(ext_int_pos_z ~ mean_alpha_pos_z  + b + mean_beta_z + (1 | uid), data=posts_r_2, 
           weights = 1/sd_alpha_pos)
summary(lm)
anova(lm)

# for the global-specific dimension
lm <- lmer(spec_glob_pos_z ~ mean_alpha_pos_z  + b + mean_beta_z + (1 | uid), data=posts_r_2, 
           weights = 1/sd_alpha_pos)
summary(lm)
anova(lm)
```

2. NLP class label probabilities

```{r nlp3}
data_ft_class <- read.csv(file = paste0(task_ver,
                                        "-learning-task-free-text-descriptions-classified-anon.csv"))

data_ft_class_long <- data_ft_class %>%
  dplyr::select(uid, contains("classifier")) %>%
  melt(id.vars="uid") %>%
  separate(variable, into=c("scenario", "valence", "classifier", "label"), sep="_", extra="drop") %>%
  mutate(valence=recode(valence, `neg`="negative",`pos`="positive"),
         label = recode(label, 
                        `myself` = "myself",
                        `other.people` = "other people",
                        `specific.situations` = "specific situations",
                        `in.general` = "in general"
                        ))
data_ft_class_long$label <- factor(data_ft_class_long$label, 
                                levels=c("myself","other people","specific situations","in general"))

rp1 <- data_ft_class_long %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learn_cond == "causal") %>%
  ggplot(aes(x=classifier, y=value, group=interaction(scenario,label), fill=label, 
             colour=label)) +
       geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
       geom_point(position=position_jitter(width=.1), size = 1, shape = 20) +
       #geom_line(aes(group=uid), colour="grey", alpha = .4) +
       geom_boxplot(outlier.shape = NA, alpha = .7, width = .3, colour = "black") +
       theme_minimal() + ylim(0,1) +
       labs(x="causal learning task", y="classifier label score for the free-text description") +
       theme(legend.title = element_blank()) + theme(legend.position="top") +
  facet_grid(rows=vars(scenario), cols=vars(valence))
rp1

rp1 <- data_ft_class_long %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learn_cond == "control") %>%
  ggplot(aes(x=classifier, y=value, group=interaction(scenario,label), fill=label, 
             colour=label)) +
       geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
       geom_point(position=position_jitter(width=.1), size = 1, shape = 20) +
       #geom_line(aes(group=uid), colour="grey", alpha = .4) +
       geom_boxplot(outlier.shape = NA, alpha = .7, width = .3, colour = "black") +
       theme_minimal() + ylim(0,1) +
       labs(x="control learning task", y="classifier label score for the free-text description") +
       theme(legend.title = element_blank()) + theme(legend.position="top") +
  facet_grid(rows=vars(scenario), cols=vars(valence))
rp1
```

```{r ft_lm}
# learning training task only
data_ft_class_long3 <- data_ft_class_long %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learnCond=="causal") %>%
  filter(label=="myself" | label=="other people")

# internal-external dimension:
lm <- lmer(value ~ scenario * valence * label + (1 | uid), 
           data=data_ft_class_long3)
summary(lm)
anova(lm)

data_ft_class_long4 <- data_ft_class_long %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learnCond=="causal") %>%
  filter(label=="in general" | label=="specific situations")

# global-specific dimension:
lm <- lmer(value ~ scenario * valence * label + (1 | uid), 
           data=data_ft_class_long4)
summary(lm)
anova(lm)

# both tasks together
data_ft_class_long5 <- data_ft_class_long %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(label=="myself" | label=="other people")
# internal-external dimension
lm <- lmer(value ~ scenario * valence * label * learnCond + (1 | uid), 
           data=data_ft_class_long5)
summary(lm)
anova(lm)

data_ft_class_long5 <- data_ft_class_long %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(label=="in general" | label=="specific situations")
# global-specific dimension
lm <- lmer(value ~ scenario * valence * label * learnCond + (1 | uid), 
           data=data_ft_class_long5)
summary(lm)
anova(lm)
```

```{r ft_corr}
data_ft_wider_all <- data_ft_class_long %>%  
  dplyr::select(-classifier) %>%
  pivot_wider(id_cols = c("uid", "scenario"), values_from = c("value"), 
              names_from = c("valence", "label")) %>%
  arrange(uid)

# get key linking numeric sequential IDs in model outtput back to original prolific identifiers
ID_trans <- data_long %>%
  dplyr::select(uid, ID) %>%
  distinct()

# merge ft into learning task posteriors data
model <-  "m_qlearning2_negpos_2alpha_2q0i1_2q0i23_bylearn"
data_to_fit <- "b123"
fit <- readRDS(file = paste0("./stan-fits/", model, "-", data_to_fit, "-", task_ver, "-fit.rds"))

params <- c("alpha_pos", "beta")

learning_posts <- as.data.frame(summary(fit, pars=params)$summary) %>%
  dplyr::select(mean,sd) %>%
  add_rownames(var = "var") %>%
  separate(var, sep="\\[", into=c("parameter","ID"), remove=TRUE, extra="drop") %>%
  separate(ID, sep=-1, into="ID", extra="drop") %>%
  pivot_wider(id_cols = "ID", names_from = "parameter", values_from = c("mean", "sd"))

posts_uids <- merge(learning_posts, ID_trans, by="ID")
posts_ft <- merge(posts_uids, data_ft_wider_all, by = "uid") %>%
  mutate(learn_cond = ifelse(ID %in% learn_control_IDs, "control", "causal"))
  

p <- posts_ft %>%
  filter(learn_cond=="causal") %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  ggplot(aes(x=mean_alpha_pos, y=positive_myself, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") + ylim(0,1.1) +
  theme_minimal() + theme(legend.position="none") + facet_grid(cols = vars(scenario))
p

p <- posts_ft %>%
  filter(learn_cond=="causal") %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  ggplot(aes(x=mean_alpha_pos, y=`positive_other people`, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") + ylim(0,1.1) +
  theme_minimal() + theme(legend.position="none") + facet_grid(cols = vars(scenario))
p

p <- posts_ft %>%
  filter(learn_cond=="causal") %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  ggplot(aes(x=mean_alpha_pos, y=`positive_in general`, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") + ylim(0,1.1) +
  theme_minimal() + theme(legend.position="none") + facet_grid(cols = vars(scenario))
p

p <- posts_ft %>%
  filter(learn_cond=="causal") %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  ggplot(aes(x=mean_alpha_pos, y=`positive_specific situations`, size = precision, weight=precision, 
             )) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) + ylim(0,1.1) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_grid(cols = vars(scenario))
p
```

```{r ft_lm2}
posts_r_2 <- posts_ft %>%
  mutate(b = ifelse(scenario=="scenario1", 1,
                    ifelse(scenario=="scenario2", 2, 3)),
         mean_alpha_pos_z = scale(mean_alpha_pos),
         mean_beta_z = scale(mean_beta)) %>%
  filter(learn_cond=="causal")

lm <- lmer(positive_myself ~ mean_alpha_pos_z + b + mean_beta_z + (1 | uid), data=posts_r_2, 
           weights = 1/sd_alpha_pos)
summary(lm)
anova(lm)

lm <- lmer(`positive_other people` ~ mean_alpha_pos_z + b + mean_beta_z + (1 | uid), data=posts_r_2, 
           weights = 1/sd_alpha_pos)
summary(lm)
anova(lm)

# lm <- lmer(`positive_in general` ~ mean_alpha_pos_z + b + mean_beta_z + (1 | uid), data=posts_r_2, 
#            weights = 1/sd_alpha_pos)
# summary(lm)
# anova(lm)
# 
# lm <- lmer(`positive_specific situations` ~ mean_alpha_pos_z + b + mean_beta_z + (1 | uid), data=posts_r_2, weights = 1/sd_alpha_pos)
# summary(lm)
# anova(lm)
```


Are explicit ratings and free-text class labels tapping the same information?

```{r ratings_ft_corr}
data_r_wider_all2 <- data_r_wider_all %>%
  filter(!block=="block0") %>%
  mutate(scenario = ifelse(block=="block1", "scenario1",
                           ifelse(block=="block2", "scenario2", "scenario3"))) %>%
  dplyr::select(-block)

ratings_ft_wide <- merge(data_r_wider_all2, data_ft_wider_all, by = c("uid", "scenario"))  %>%
    mutate(learn_condition = ifelse(uid %in% learn_controls, "control", "causal"))

p <- ratings_ft_wide %>%
  #filter(learn_condition=="causal") %>%
  ggplot(aes(x=ext_int_neg, y=negative_myself)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_grid(rows=vars(scenario), cols=vars(learn_condition))
p

p <- ratings_ft_wide %>%
  ggplot(aes(x=ext_int_pos, y=positive_myself)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "left", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_grid(rows=vars(scenario), cols=vars(learn_condition))
p

p <- ratings_ft_wide %>%
  ggplot(aes(x=spec_glob_neg, y=`negative_in general`)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_grid(rows=vars(scenario), cols=vars(learn_condition))
p

p <- ratings_ft_wide %>%
  ggplot(aes(x=spec_glob_pos, y=`positive_in general`)) +
  geom_point() + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_grid(rows=vars(scenario), cols=vars(learn_condition))
p
```


```{r ratings_ft_corrmat}
tmp1 <- ratings_ft_wide %>%
  mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learnCond=="causal") %>%
  dplyr::select(uid, 
                ext_int_neg, spec_glob_neg, 
                negative_myself, `negative_in general`) %>%
  group_by(uid) %>%
  summarise_at(vars(-group_cols()), mean) %>%
  dplyr::select(-uid)

corr1 <- round(cor(tmp1), 2)
ggcorrplot(corr1, hc.order = TRUE, type = "upper", outline.col = "white",
             colors = c("#6D9EC1", "white", "#E46726"))

tmp2 <- ratings_ft_wide %>%
   mutate(learnCond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  filter(learnCond=="causal") %>%
  dplyr::select(uid, 
                ext_int_pos, spec_glob_pos,
                positive_myself, `positive_in general`) %>%
  group_by(uid) %>%
  summarise_at(vars(-group_cols()), mean) %>%
  dplyr::select(-uid)

corr2 <- round(cor(tmp2), 2)
ggcorrplot(corr2, hc.order = TRUE, type = "upper", outline.col = "white",
             colors = c("#6D9EC1", "white", "#E46726"))
```


```{r explor_regress}
posts_r2 <- posts_r %>%
  filter(!block=="block0") %>%
  mutate(scenario = ifelse(block=="block1", "scenario1",
                           ifelse(block=="block2", "scenario2", "scenario3"))) %>%
  dplyr::select(-block)

posts_r_ft <- merge(posts_r2, posts_ft, by=c("uid", "ID", "scenario",
                                             "mean_alpha_pos", 
                                             "sd_alpha_pos", 
                                             "mean_beta"
                                             )) %>%
  dplyr::select(-scenario) %>%
  mutate(ID = as.numeric(ID)) %>%
  group_by(uid) %>%
  summarise_all(.funs = "mean") %>%
  mutate(ext_int_pos_z = scale(ext_int_pos),
         spec_glob_pos_z = scale(spec_glob_pos),
         pos_myself_z = scale(positive_myself),
         pos_other.people_z = scale(`positive_other people`),
         pos_in.general_z = scale(`positive_in general`),
         pos_specific.situations_z = scale(`positive_specific situations`))

tmp_c <- posts_r_ft %>%
  filter(ID %in% learn_control_IDs) %>%
  dplyr::select(mean_alpha_pos, mean_beta, ext_int_pos, spec_glob_pos, pos_myself, pos_in.general)
sds <- posts_r_ft %>%
  filter(ID %in% learn_control_IDs)

weighted_corr <- cov.wt(tmp_c, wt = 1/sds$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor
ggcorrplot(corr, hc.order = FALSE, type = "upper", outline.col = "white", 
             colors = c("#6D9EC1", "white", "#E46726"))
# ggsave(filename = paste0("./figures/", task_ver, "-corr-plot-params-ratings-nlp-control.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

tmp_caus <- posts_r_ft %>%
  filter(!ID %in% learn_control_IDs) %>%
  dplyr::select(mean_alpha_pos, mean_beta, ext_int_pos, spec_glob_pos, pos_myself, pos_in.general)
sds2 <- posts_r_ft %>%
  filter(!ID %in% learn_control_IDs)
weighted_corr <- cov.wt(tmp_caus, wt = 1/sds2$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor
ggcorrplot(corr, hc.order = FALSE, type = "upper", outline.col = "white", 
             colors = c("#6D9EC1", "white", "#E46726"))
# ggsave(filename = paste0("./figures/", task_ver, "-corr-plot-params-ratings-nlp-causal.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)
```

And are they related to self-reported demographic or clinical information?

```{r ratings_ft_demogs}
# load data
data_quest_wide <- read.csv(file=paste0(task_ver, "-self-report-data-anon.csv")) %>%
  dplyr::select(-X)

# rescore a few variables
data_quest_wide_all <- data_quest_wide %>%
  mutate(age_z = scale(demogs_age),
         gender01 = ifelse(demogs_gender=="man",0,1),
         gender01_z = scale(gender01),
         demogs_tx01 = ifelse(grepl("yes", demogs_tx), 1, 0),
         demogs_tx01_z = scale(demogs_tx01),
         demogs_talking = ifelse(grepl("talking therapy", demogs_tx), 1, 0),
         demogs_talking_z = scale(demogs_talking),
         neurodiv01 = ifelse(grepl("yes", demogs_neurodiv), 1, 0),
         neurodiv01_z = scale(neurodiv01),
         PHQ9_total_z = scale(PHQ9_total),
         miniSPIN_total_z = scale(miniSPIN_total),
         DAS_total_z = scale(DAS_total)
         )

posts_r_ft_demogs <- merge(posts_r_ft, data_quest_wide_all, by="uid") %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal"))

# for learning training task participants
tmp2 <- posts_r_ft_demogs %>%
  filter(!ID %in% learn_control_IDs) %>%
  dplyr::select(mean_alpha_pos, age_z, gender01_z, neurodiv01_z, demogs_tx01_z, demogs_talking_z,
                PHQ9_total_z, DAS_total_z, miniSPIN_total_z)
sds <- posts_r_ft_demogs %>%
  filter(!ID %in% learn_control_IDs)

weighted_corr <- cov.wt(tmp2, wt = 1/sds$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor
ggcorrplot(corr, hc.order = FALSE, type = "upper", outline.col = "white", 
             colors = c("#6D9EC1", "white", "#E46726"))
# ggsave(filename = paste0("./figures/", task_ver, "-corr-plot-demogs-causal.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

# for control learning task participants
tmp2 <- posts_r_ft_demogs %>%
  filter(ID %in% learn_control_IDs) %>%
  dplyr::select(mean_alpha_pos, age_z, gender01_z, neurodiv01_z, demogs_tx01_z, demogs_talking_z,
                PHQ9_total_z, DAS_total_z, miniSPIN_total_z)
sds <- posts_r_ft_demogs %>%
  filter(ID %in% learn_control_IDs)

weighted_corr <- cov.wt(tmp2, wt = 1/sds$sd_alpha_pos, cor = TRUE)
corr <- weighted_corr$cor
ggcorrplot(corr, hc.order = FALSE, type = "upper", outline.col = "white", 
             colors = c("#6D9EC1", "white", "#E46726"))
# ggsave(filename = paste0("./figures/", task_ver, "-corr-plot-demogs-control.svg"),
#        plot = last_plot(), device = "svg", dpi = 300)

# plot some bivariate relationships:
a <- posts_r_ft_demogs %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  ggplot(aes(x=PHQ9_total, y=mean_alpha_pos, size = precision, weight = precision)) +
  geom_point(colour="lightcoral") + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, color="grey", formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_wrap(~learn_cond)

b <- posts_r_ft_demogs %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  ggplot(aes(x=demogs_age, y=mean_alpha_pos, size = precision, weight = precision)) +
  geom_point(colour="deepskyblue") + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, color="grey", formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_wrap(~learn_cond)

c <- posts_r_ft_demogs %>%
  dplyr::select(uid, mean_alpha_pos, gender01) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  mutate(gender = ifelse(gender01==0, "man", "woman/non-binary/other")) %>%
  ggplot(aes(x=gender, y=mean_alpha_pos, group=gender,
             fill=gender, colour=gender)) +
       geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
       geom_point(position=position_jitter(width=.02), size = 3, shape = 20) +
       geom_boxplot(outlier.shape = NA, alpha = .5, width = .3, colour = "black") +
       theme_minimal() + labs(x="gender", y="") + 
       theme(legend.title = element_blank()) + theme(legend.position = "none") +
       scale_fill_manual(values=wes_palette(n=2, name="Moonrise3")) +
       scale_colour_manual(values=wes_palette(n=2, name="Moonrise3"))  + facet_wrap(~learn_cond)

d <- posts_r_ft_demogs %>%
  dplyr::select(uid, mean_alpha_pos, demogs_talking) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  mutate(talking_tx = ifelse(demogs_talking==0, "no", "yes")) %>%
  ggplot(aes(x=talking_tx, y=mean_alpha_pos, group=talking_tx,
             fill=talking_tx, colour=talking_tx)) +
       geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
       geom_point(position=position_jitter(width=.02), size = 3, shape = 20) +
       geom_boxplot(outlier.shape = NA, alpha = .5, width = .3, colour = "black") +
       theme_minimal() + labs(x="previous talking therapy treatment", y="") + 
       theme(legend.title = element_blank()) + theme(legend.position = "none") +
       scale_fill_manual(values=wes_palette(n=2, name="Moonrise3")) +
       scale_colour_manual(values=wes_palette(n=2, name="Moonrise3")) + facet_wrap(~learn_cond)

e <- posts_r_ft_demogs %>%
  dplyr::select(uid, mean_alpha_pos, neurodiv01) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  mutate(nd = ifelse(neurodiv01==0, "no", "yes")) %>%
  ggplot(aes(x=nd, y=mean_alpha_pos, group=nd,
             fill=nd, colour=nd)) +
       geom_flat_violin(position=position_nudge(x = .1, y = 0),  
                        adjust = 1.5, trim = TRUE, alpha = .4, colour = NA) +
       geom_point(position=position_jitter(width=.02), size = 3, shape = 20) +
       geom_boxplot(outlier.shape = NA, alpha = .5, width = .3, colour = "black") +
       theme_minimal() + labs(x="self-reported neurodivergence", y="") + 
       theme(legend.title = element_blank()) + theme(legend.position = "none") +
       scale_fill_manual(values=wes_palette(n=2, name="Moonrise3")) +
       scale_colour_manual(values=wes_palette(n=2, name="Moonrise3")) + facet_wrap(~learn_cond)

f <- posts_r_ft_demogs %>%
  mutate(precision = 1/sd_alpha_pos) %>%
  mutate(learn_cond = ifelse(uid %in% learn_controls, "control", "causal")) %>%
  ggplot(aes(x=miniSPIN_total, y=mean_alpha_pos, size = precision, weight = precision)) +
  geom_point(colour="lightcoral") + 
  geom_smooth(method = "lm", se=TRUE, alpha=.2, color="grey", formula = y~x) +
  stat_poly_eq(formula = y~x,
               aes(label = paste(stat(rr.label), stat(p.value.label), sep = "~~~")),
               parse = TRUE,
               label.x = "right", label.y = "top") +
  theme_minimal() + theme(legend.position="none") + facet_wrap(~learn_cond)

((b + c) /
 (a + d) /
 (f + e))
```
